{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25cd528a-260d-411c-95b2-872e38b45990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f795925e-1fdc-40c4-8c72-829bad9c787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, folder_path):\n",
    "        \"\"\"\n",
    "        \n",
    "        Initializing neccessary atribute and reading LIAR dataset from the folder directory.\n",
    "        LIAR dataset includes:\n",
    "            train.tsv - training set.\n",
    "            valid.tsv - validation set.\n",
    "            test.tsv - testing set.\n",
    "        **Because this experiment do not need valid set, I will concatenate it with training set.**\n",
    "\n",
    "        Freatures of this dataset includes:\n",
    "            ['ID', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'affiliation',\n",
    "             'barely-true', 'false', 'half-true', 'mostly-true', 'pant-on-fire', 'context'] \n",
    "             \n",
    "        - [statement]      is the text we need to categorize.\n",
    "        - [label]          is target.\n",
    "        - [Others feature] is meta-data of the statement.\n",
    "        - [ID]             is the id of the statement (not neccessary).\n",
    "        \n",
    "        ---\n",
    "        Inputs :\n",
    "            folder_path: A string contain path to dataset.\n",
    "        Output :\n",
    "            _\n",
    "            \n",
    "        \"\"\"\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        \n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        self.features = ['ID', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'affiliation',\n",
    "         'barely-true', 'false', 'half-true', 'mostly-true', 'pant-on-fire', 'context']\n",
    "        self.train = pd.concat([pd.read_csv(folder_path+'/train.tsv', delimiter='\\t', names=self.features, quoting=3),\n",
    "                                pd.read_csv(folder_path+'/valid.tsv', delimiter='\\t', names=self.features, quoting=3)],\n",
    "                                ignore_index=True)\n",
    "        self.test = pd.read_csv(folder_path+'/test.tsv', delimiter='\\t', names=self.features, quoting=3)\n",
    "        self.y_train = self.train['label']\n",
    "        self.y_test = self.test['label']\n",
    "        self.target = 'label'\n",
    "        self.liar_preprocess()\n",
    "\n",
    "    def subtract_current_credit(self, row):\n",
    "        \"\"\"\n",
    "        \n",
    "        Subtract the current label from the credit history of current statement.\n",
    "            \n",
    "        \"\"\"\n",
    "        label = row['label'] \n",
    "        try:\n",
    "            row[label] -= 1  \n",
    "        except:\n",
    "            pass\n",
    "        return row\n",
    "            \n",
    "    def liar_preprocess(self):\n",
    "        \"\"\"\n",
    "    \n",
    "        Preproces steps of LIAR dataset which is include:\n",
    "            * Define which are text features\n",
    "            * Define which are numeric features (credit history)\n",
    "            * Fill the blanks (missing cell) of text features with word [unknow]\n",
    "            * Prevent data leakage from numeric features\n",
    "                According to dataset author: \n",
    "                    \"Credit history include the count of the current statement, \n",
    "                    it is important to subtract the current label from the credit history when using this \n",
    "                    meta data vector in prediction experiments.\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.text_features = ['statement', 'subject', 'speaker', 'job', 'state', 'affiliation', 'context']\n",
    "        self.num_features = ['barely-true', 'false', 'half-true', 'mostly-true', 'pant-on-fire']\n",
    "        self.train[self.text_features] = self.train[self.text_features].fillna(\"unknow\").astype(str)\n",
    "        self.test[self.text_features] = self.test[self.text_features].fillna(\"unknow\").astype(str)\n",
    "\n",
    "        self.train = self.train.apply(self.subtract_current_credit, axis=1)\n",
    "        self.test = self.test.apply(self.subtract_current_credit, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e687c68-a2c3-4277-b2d3-9b4e06bbbb40",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056dfa05-46cf-411d-8cb3-6333567d2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "PUNCT_TRANS = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def text_preprocess(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    Clean statement feature.\n",
    "    \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(PUNCT_TRANS)\n",
    "    text = ' '.join([word for word in word_tokenize(text) if word not in STOPWORDS])\n",
    "    return text\n",
    "\n",
    "def context_preprocess(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    Clean context feature.\n",
    "    \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('e mail|e-mail|email|mailer','mail', text)\n",
    "    text = re.sub('television','tv', text)\n",
    "    text = re.sub('website','web', text)\n",
    "    text = text.translate(PUNCT_TRANS)\n",
    "    text = ' '.join([word for word in word_tokenize(text) if word not in STOPWORDS])\n",
    "    return text\n",
    "    \n",
    "def subject_preprocess(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    Clean subject feature.\n",
    "    \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split(','))\n",
    "    return text\n",
    "\n",
    "def job_preprocess(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    Clean job feature.\n",
    "    \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(PUNCT_TRANS)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ddbca6-13a2-4759-84d0-22d5a480eec6",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48a5de9-030d-497d-a59c-5ec69c508f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fuzzifier():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        A model calculate centroids of training set (according to its labels).\n",
    "        After fitting with training data, this model can create fuzzy sets (membership values matrixs) for new data points\n",
    "        \n",
    "        \"\"\"\n",
    "        self.centroids = None\n",
    "        \n",
    "    def __calculate_centroids(self, data, label, targets):\n",
    "        \"\"\"\n",
    "        \n",
    "        Calculate centroids of data\n",
    "        ---\n",
    "        Inputs :\n",
    "            data : A matrix prepresent list of data points.\n",
    "            label : A list contains labels of each row in data.\n",
    "            targets : A list contains \n",
    "        Output :\n",
    "            A matrix. Each column of matrix is a vector prepresent a centroid of data.\n",
    "    \n",
    "        \"\"\"\n",
    "        centroids = np.zeros((len(targets), data.shape[1])) \n",
    "        for i, target in enumerate(targets):\n",
    "            cluster_data = data[label == target]\n",
    "            centroids[i, :] = np.mean(cluster_data, axis=0) \n",
    "        return centroids\n",
    "    def __estimateDistances(self, x, V):\n",
    "        \"\"\"\n",
    "        \n",
    "        Estimating Euclid distances from x to V.\n",
    "        ---\n",
    "        Inputs :\n",
    "            x : A vector prepresent a object (row) in dataset.\n",
    "            v : A matrix prepresent list of centroids.\n",
    "        Output :\n",
    "            A list of Euclid distances from x to V.\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        distances = np.empty(len(V))\n",
    "        for i in range(len(V)):\n",
    "            distances[i] = np.power(norm(x - V[i]), 2)\n",
    "        return distances\n",
    "    \n",
    "    def __getMembershipValue(self, X, V, c, m = 4):\n",
    "        \"\"\"\n",
    "        \n",
    "        Calculate membership values of data (X) to centroids (V).\n",
    "        ---\n",
    "        Inputs :\n",
    "            X : A matrix prepresent list of data points.\n",
    "            V : A matrix prepresent list of centroids.\n",
    "            c : Number of centroids.\n",
    "            m : Fuzzy parameter\n",
    "        Output :\n",
    "            A matrix prepresent membership values of data (X) to centroids (V).\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        U = np.empty((X.shape[0], c))\n",
    "        p = float(2/(1-m))\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i].toarray()\n",
    "            distances = self.__estimateDistances(x, V)\n",
    "            \n",
    "            if (distances.min() == 0.0):\n",
    "                for j in range(c):\n",
    "                    U[i][j] = 0\n",
    "                U[i][np.argmin(distances)] = 1\n",
    "                continue    \n",
    "                \n",
    "            U[i] = np.power(distances, p)\n",
    "            Sum = np.sum(U[i])\n",
    "            U[i] = U[i]/Sum\n",
    "        return U\n",
    "\n",
    "    def fit(self, X, y, n_class):\n",
    "        \"\"\"\n",
    "        \n",
    "        Initialize centroids of the giving data.\n",
    "        ---\n",
    "        Inputs :\n",
    "            X : A matrix prepresent list of data points.\n",
    "            y : A list contains labels of each row in data.\n",
    "            n_class : The number of labels.\n",
    "        Output :\n",
    "            -\n",
    "        \"\"\"\n",
    "        self.centroids = self.__calculate_centroids(X, y, range(n_class))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Calculate fuzzy set (membership values matrix) of the giving data.\n",
    "        ---\n",
    "        Inputs :\n",
    "            X : A matrix prepresent list of data points.\n",
    "        Output :\n",
    "            A matrix prepresent membership values of data (X) according to model's centroids.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.__getMembershipValue(X=X, V=self.centroids, c=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
